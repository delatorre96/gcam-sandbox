netcdffolder <- "spatial_landcover_netcdf"
nc_file <- paste0("_demeter_",scenario_name,"_",output_year[j,1],".nc")
NetCDFfiles_path <- file.path(dir_demeter,scenario_name,netcdffolder,nc_file)
print(NetCDFfiles_path)
# Read the nc file from Demeter outputs
ncin <- nc_open(NetCDFfiles_path)
# Set the list of variables
variables = names(ncin$var)
# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)
lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)
lonlat <- as.matrix(expand.grid(lon, lat))
dim(lonlat)
# Function to create dataframes from the variables of the nc file
for (i in 1:length(variables)) {
# i = 5
land_df <- na.omit(data.frame(cbind(lonlat, as.vector(ncvar_get(ncin, ncin$var[[i]])))))
names(land_df) <- c("lon", "lat", paste(ncin$var[[i]]$longname)) # instead of land, the longname of the variable i
# assign(paste(ncin$var[[i]]$longname, "df", sep="_"), land_df)
merge_df = merge(merge_df, land_df)
print(ncin$var[[i]]$longname)
}
# Now create a new column with the sum of the columns that finish by "irrigated"
merge_df$crop_irr = rowSums(merge_df[, grepl("irrigated", names(merge_df))])
merge_df$crop_rfd = rowSums(merge_df[, grepl("rainfed|otherarableland", names(merge_df))])
merge_df = merge_df %>%
dplyr::select(!contains("irrigated")) %>%
dplyr::select(!contains("rainfed")) %>%
dplyr::select(!contains("otherarableland")) %>%
dplyr::select(!c(basin_id, region_id, water))
# assign(paste(ncin$var[[i]]$longname, "df", sep="_"), merge_df)
assign(paste("merge", j, "df", sep="_"), merge_df)
}
print(paste0("LUC dataframe for scenario ", scenario_name, " created from Demeter outputs"))
# Merge 2020 and 2050
merge_df = merge(merge_1_df, merge_2_df, by = c("lon", "lat"))
# Geography parameters
areas_land_types <- merge_df %>% rename ("longitude"="lon") %>% rename ("latitude"="lat")
areas_land_types_shp <- areas_land_types
coordinates(areas_land_types_shp)=~longitude+latitude
proj4string(areas_land_types_shp)<- CRS("+proj=longlat +datum=WGS84")
areas_land_types_shp = st_as_sf(areas_land_types_shp)
# Unique geometries
unique(st_geometry_type(areas_land_types_shp$geometry))
# Check the classes
class(areas_land_types_shp)
crs(areas_land_types_shp)
areas_land_types_shp = st_transform(areas_land_types_shp, crs=4326)
# Check the geometries
areas_land_types_valid <- st_is_valid(areas_land_types_shp)
# Identify invalid geometries
invalid_areas_land_types <- areas_land_types_shp[!areas_land_types_valid, ]
# Join attributes by location
joined_shp_id = st_join(areas_land_types_shp, ecoregions_id, join = st_intersects) # way too long, need to simply both with index
# Aggregate per land , dropping geometry
joined_shp_agg <- joined_shp_id %>%
st_drop_geometry() %>%
group_by(OBJECTID) %>%
summarise(
forest.x = sum(forest.x),
pasture.x = sum(pasture.x),
crop_irr.x = sum(crop_irr.x),
crop_rfd.x = sum(crop_rfd.x),
forest.y = sum(forest.y),
pasture.y = sum(pasture.y),
crop_irr.y = sum(crop_irr.y),
crop_rfd.y = sum(crop_rfd.y),
) %>%
ungroup() %>%
full_join(ecoregions_shp[,c(1,18)]) %>%
rename(ECOREGION_CODE = eco_code) %>%
mutate_at(vars(-ECOREGION_CODE), ~replace(., is.na(.), 0))
# Incorporate the ecoregions names from Chaudhary and Brookes (2018) to the Results matrix obtained from QGIS with data on land  type area changes over time
# Conversion function from square km to square meters
sqm <- function(x, na.rm = FALSE) (x*1000000)
joined_shp_extended <- joined_shp_agg %>%
merge(ecoregions_ID,by="ECOREGION_CODE") %>%
relocate(ECO_NAME, .after=ECOREGION_CODE) %>%
group_by(ECOREGION_CODE,ECO_NAME) %>%
summarise(across(where(is.numeric), sum)) %>%
dplyr::select(-OBJECTID) %>%
mutate_if(is.numeric, sqm, na.rm = FALSE) %>%
ungroup() %>%
as.data.frame() %>%
mutate(
forest.diff = forest.y - forest.x,
pasture.diff = pasture.y - pasture.x,
crop_irr.diff = crop_irr.y - crop_irr.x,
crop_rfd.diff = crop_rfd.y - crop_rfd.x,
)
# Estimate the final PSL number with CF file
CF = read.csv("gcam_sdg/data/CF.csv")
final = merge(joined_shp_extended, CF, by = c("ECOREGION_CODE")) %>%
mutate(
forest_PSL = forest.diff * Forest_CF,
pasture_PSL = pasture.diff * Pasture_CF,
crop_irr_PSL = crop_irr.diff * Irrigated_crop_CF,
crop_rfd_PSL = crop_rfd.diff * Rainfed_crop_CF,
) %>%
mutate(final_PSL = rowSums(across(ends_with("_PSL"))))
# Aggregate across the ecoregions and compute final PSL across land s
final_agg = final %>%
summarize(
forest_PSL = sum(forest_PSL),
pasture_PSL = sum(pasture_PSL),
crop_irr_PSL = sum(crop_irr_PSL),
crop_rfd_PSL = sum(crop_rfd_PSL),
final_PSL = sum(final_PSL)
) %>%
mutate(scenario = scenario_name)
# Delete the first column X
final_agg = final_agg[,-1]
# write.xlsx(final_agg,paste0(dipc_path,"results/PSL-results/",scenario_name,"_PSL_2020_2050.xlsx"), overwrite = TRUE, rowNames=TRUE, colNames=TRUE)
if (saveOutput) write.csv(final_agg,paste0("/scratch/bc3lc/GCAM_v7p1_plus/gcam_sdg/output/SDG15-Land/results/PSL-results/",scenario_name,"_PSL_2020_2050.csv"), row.names = F)
print(paste0("PSL dataframe for scenario ", scenario_name, " saved in results"))
final_csv <- rbind(final_csv, final_agg)
}
# Write CSV
if (saveOutput) write.csv(final_csv, file = file.path('gcam_sdg/output/SDG15-Land/results/PSL-prj-results/',paste0('PSL_',gsub("\\.dat$", "", prj_name, ".csv"))), row.names = F)
print(paste0("PSL dataframe for all scenarios of the ", prj_name, " saved in results"))
return(invisible(final_csv))
}
library(rgcam)
pathToDbs <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output"
my_gcamdb_basexdb <- "database_basexdb"
conn <- localDBConn(pathToDbs, my_gcamdb_basexdb)
scenariosAnalyze<-c("Reference","policy_carbon")
myQueryfile  <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output/myQueries_try.xml"
prj1 <- addScenario(conn = conn, proj = 'myProject.dat', scenario  = scenariosAnalyze, queryFile = myQueryfile)
scenarios <- listScenarios(prj1)
data <- get_sdg15_land_indicator(prj1)
library(reticulate)
conda_list()
data <- get_sdg15_land_indicator(prj1)
warnings()
getwd(
)
library(rgcam)
pathToDbs <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output"
my_gcamdb_basexdb <- "database_basexdb"
conn <- localDBConn(pathToDbs, my_gcamdb_basexdb)
scenariosAnalyze<-c("Reference","policy_carbon")
myQueryfile  <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output/allQueries.xml"
prj1 <- addScenario(conn = conn, proj = 'myProject.dat', scenario  = scenariosAnalyze, queryFile = myQueryfile)
scenarios <- listScenarios(prj1)
queries <- listQueries(prj1, 'policy_carbon')
queries2 <- listQueries(prj1, 'Reference')
queries
inicio <- Sys.time()
library(rgcam)
pathToDbs <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output"
my_gcamdb_basexdb <- "database_basexdb"
conn <- localDBConn(pathToDbs, my_gcamdb_basexdb)
scenariosAnalyze<-c("Reference","policy_carbon")
myQueryfile  <- "C:/Users/ignacio.delatorre/Documents/gcam-v8.2-Windows-Release-Package/output/allQueries.xml"
prj1 <- addScenario(conn = conn, proj = 'myProject.dat', scenario  = scenariosAnalyze, queryFile = myQueryfile)
scenarios <- listScenarios(prj1)
queries <- listQueries(prj1, 'policy_carbon')
queries2 <- listQueries(prj1, 'Reference')
detailed_land_allocation <- getQuery(prj1,"detailed land allocation"  )
fin <- Sys.time()
tiempo <- fin - inicio
print(tiempo)
library(DiagrammeR)
setwd("C:/Users/ignacio.delatorre/Documents/Understanding GCAM/gcam-core/input/gcamdata")
devtools::load_all()
setwd('C:/Users/ignacio.delatorre/Documents/Understanding GCAM/gcamdata_trees')
source('functions_drawTree.R')
source('functions_manipulateCSV.R')
get_all_initial_files <- function(tree){
all_initial_files <- unlist(lapply(tree, function(x) {
inputs <- x$inputs
inputs[grepl("/", inputs)]
}))
all_initial_files <- unname(all_initial_files)
return(all_initial_files)
}
build_csv_relations <- function(tree,initial_files) {
# Para almacenar pares de CSV relacionados
relations <- data.frame(csv1 = character(0), csv2 = character(0), stringsAsFactors = FALSE)
for (script_name in names(tree)) {
inputs <- tree[[script_name]]$inputs
csv_inputs <- inputs[inputs %in% initial_files]
# Si hay al menos 2 CSV iniciales en los inputs del script, crear pares
if (length(csv_inputs) > 1) {
combos <- t(combn(csv_inputs, 2))  # pares sin repetición
df_pairs <- data.frame(csv1 = combos[,1], csv2 = combos[,2], stringsAsFactors = FALSE)
relations <- rbind(relations, df_pairs)
}
}
# Opcional: quitar duplicados porque un par puede salir varias veces
relations <- unique(relations)
return(relations)
}
draw_csv_graph <- function(relations) {
if (nrow(relations) == 0) {
message("No hay relaciones para dibujar.")
return(NULL)
}
# Generar aristas en formato "csv1" -> "csv2"
edges <- sprintf('"%s" -- "%s"', relations$csv1, relations$csv2)  # -- para grafo no dirigido
edges_str <- paste(unique(edges), collapse = ";\n  ")
dot <- sprintf("graph CSV_Relations {\n  rankdir=LR;\n  %s;\n}", edges_str)
gr <- grViz(dot)
return(gr)
}
get_csv_info <- function(csv_file) {
dir_iniciar <- getwd()
on.exit(setwd(dir_iniciar), add = TRUE)
setwd(dir_csvs_iniciales)
path <- find_csv_file(csv_file, optional = TRUE)
lines <- readLines(path)
header_lines <- lines[grepl("^#", lines)]
df <- read.csv(path, comment.char = '#', check.names = FALSE)
return(list(header_lines = header_lines, df = df, path = path))
}
clean_dbml_column_name <- function(name) {
clean_name <- gsub("[^a-zA-Z0-9_]", "_", name)
clean_name <- gsub("_+", "_", clean_name)
clean_name <- gsub("^_|_$", "", clean_name)
if (nchar(clean_name) == 0) {
clean_name <- "col_unknown"
}
# Si empieza con número o contiene caracteres no permitidos, o era solo números, la dejamos entre comillas dobles
needs_quotes <- FALSE
if (grepl("^[0-9]", clean_name) || grepl("^[0-9]+$", clean_name)) {
needs_quotes <- TRUE
}
# Además, si el original tenía espacios o caracteres especiales, usar comillas para seguridad
if (grepl("[^a-zA-Z0-9_]", name)) {
needs_quotes <- TRUE
}
if (needs_quotes) {
# Comillas dobles, y escapamos si hubiera comillas internas
clean_name <- paste0('"', gsub('"', '\\"', clean_name), '"')
}
return(clean_name)
}
clean_dbml_table_name <- function(name) {
# Igual limpieza para tablas, que evita caracteres raros
clean_name <- gsub("[^a-zA-Z0-9_]", "_", name)
clean_name <- gsub("_+", "_", clean_name)
clean_name <- gsub("^_|_$", "", clean_name)
if (nchar(clean_name) == 0) {
clean_name <- "table_unknown"
}
if (grepl("^[0-9]", clean_name) || grepl("^[0-9]+$", clean_name)) {
clean_name <- paste0("table_", clean_name)
}
return(clean_name)
}
build_dbml_from_pairs <- function(pairs) {
tables <- list()
relations <- list()
clean_name <- function(name) {
gsub("[^a-zA-Z0-9_]", "_", basename(name))
}
escape_colname <- function(col) {
if (grepl("^[0-9]", col) || grepl("[^a-zA-Z0-9_]", col)) {
paste0("\"", col, "\"")
} else {
col
}
}
get_dbml_type <- function(column) {
cls <- class(column)[1]
if (cls %in% c("integer", "integer64")) {
return("integer")
} else if (cls %in% c("numeric", "double")) {
return("float")
} else if (cls %in% c("logical")) {
return("boolean")
} else if (inherits(column, "Date")) {
return("date")
} else if (inherits(column, "POSIXct") || inherits(column, "POSIXlt")) {
return("timestamp")
} else {
return("varchar")
}
}
for (i in seq_len(nrow(pairs))) {
csv1 <- pairs$csv1[i]
csv2 <- pairs$csv2[i]
df1 <- tryCatch(get_csv_info(csv1)$df, error = function(e) {
message(sprintf("ERROR al cargar '%s': %s", csv1, e$message))
return(NULL)
})
df2 <- tryCatch(get_csv_info(csv2)$df, error = function(e) {
message(sprintf("ERROR al cargar '%s': %s", csv2, e$message))
return(NULL)
})
if (is.null(df1) || is.null(df2)) {
message(sprintf("Saltando par (%s, %s) por fallo en la carga.", csv1, csv2))
next
}
csv1_clean <- clean_name(csv1)
csv2_clean <- clean_name(csv2)
if (!csv1_clean %in% names(tables)) {
tables[[csv1_clean]] <- sapply(df1, get_dbml_type, USE.NAMES = TRUE)
}
if (!csv2_clean %in% names(tables)) {
tables[[csv2_clean]] <- sapply(df2, get_dbml_type, USE.NAMES = TRUE)
}
common_cols <- intersect(colnames(df1), colnames(df2))
if (length(common_cols) > 0) {
for (col in common_cols) {
relations[[length(relations) + 1]] <- list(
from_table = csv1_clean,
to_table = csv2_clean,
column = col
)
}
}
}
dbml <- ""
for (tbl in names(tables)) {
dbml <- paste0(dbml, "Table ", tbl, " {\n")
for (col in names(tables[[tbl]])) {
dbml <- paste0(dbml, "  ", escape_colname(col), " ", tables[[tbl]][col], "\n")
}
dbml <- paste0(dbml, "}\n\n")
}
seen_refs <- list()
for (rel in relations) {
from_tbl <- rel$from_table
to_tbl <- rel$to_table
col <- rel$column
if (!(from_tbl %in% names(tables))) {
warning(sprintf("Tabla '%s' no encontrada, omitiendo referencia.", from_tbl))
next
}
if (!(to_tbl %in% names(tables))) {
warning(sprintf("Tabla '%s' no encontrada, omitiendo referencia.", to_tbl))
next
}
if (!(col %in% names(tables[[from_tbl]]))) {
warning(sprintf("Columna '%s' no encontrada en tabla '%s', omitiendo referencia.", col, from_tbl))
next
}
if (!(col %in% names(tables[[to_tbl]]))) {
warning(sprintf("Columna '%s' no encontrada en tabla '%s', omitiendo referencia.", col, to_tbl))
next
}
if (from_tbl == to_tbl) next
ref_key <- paste(from_tbl, col, to_tbl, col, sep = "->")
if (!(ref_key %in% seen_refs)) {
dbml <- paste0(dbml, "Ref: ", from_tbl, ".", escape_colname(col), " > ", to_tbl, ".", escape_colname(col), "\n")
seen_refs <- c(seen_refs, ref_key)
}
}
return(list(dbml =  dbml, relations = relations, tables = tables))
}
count_related_columns_between_tables <- function(tables, relations) {
# Todas las tablas para fila y columna
all_tables <- names(tables)
# Lista para almacenar resultados (cada fila: tabla1, tabla2, num_columns, columnas_comunes)
results <- list()
# Recorrer pares de tablas (incluyendo reverso para que salga todo)
for (tbl1 in all_tables) {
for (tbl2 in all_tables) {
if (tbl1 == tbl2) next  # Opcional: saltar si no quieres comparar tabla consigo misma
# Columnas relacionadas entre tbl1 y tbl2
cols_relacionadas <- unique(unlist(lapply(relations, function(r) {
(r$from_table == tbl1 && r$to_table == tbl2) || (r$from_table == tbl2 && r$to_table == tbl1)
})))
# Columnas reales que aparecen en relaciones de tbl1-tbl2
cols <- unique(unlist(lapply(relations, function(r) {
if ((r$from_table == tbl1 && r$to_table == tbl2) || (r$from_table == tbl2 && r$to_table == tbl1)) {
return(r$column)
} else return(NULL)
})))
results[[length(results)+1]] <- list(
table_1 = tbl1,
table_2 = tbl2,
n_common_columns = length(cols),
common_columns = paste(cols, collapse = ", ")
)
}
}
# Convertir a data.frame
df <- do.call(rbind, lapply(results, as.data.frame, stringsAsFactors=FALSE))
# Convertir n_common_columns a numérico
df$n_common_columns <- as.numeric(df$n_common_columns)
return(df)
}
count_related_columns_pairwise <- function(tables, relations) {
all_tables <- names(tables)
results <- list()
for (tbl1 in all_tables) {
for (tbl2 in all_tables) {
if (tbl1 == tbl2) next
# Filtrar relaciones entre tbl1 y tbl2 (en cualquier dirección)
rels <- Filter(function(r) {
(r$from_table == tbl1 && r$to_table == tbl2) || (r$from_table == tbl2 && r$to_table == tbl1)
}, relations)
# Extraer columnas comunes de esas relaciones
cols <- unique(sapply(rels, function(r) r$column))
# Calcular porcentaje sobre total columnas de tbl1 y tbl2 (usamos mínimo para simetría)
n_common <- length(cols)
n_total_1 <- length(tables[[tbl1]])
n_total_2 <- length(tables[[tbl2]])
related_pct <- ifelse(n_common == 0, 0, 100 * n_common / min(n_total_1, n_total_2))
results[[length(results)+1]] <- list(
table_1 = tbl1,
table_2 = tbl2,
related_columns_pct = related_pct
)
}
}
df <- do.call(rbind, lapply(results, as.data.frame, stringsAsFactors=FALSE))
df$related_columns_pct <- as.numeric(df$related_columns_pct)
return(df)
}
#######Tree#######
zenergyTree <- build_tree_module(modules = 'zenergy')
ancestors <- find_ancestors (tree = zenergyTree, target = 'transportation_UCD_CORE.xml')
initial_files <- get_all_files(ancestors, zenergyTree)
#######Get some connected files#######
files_BEV <- c()
for (i in initial_files){
df <- data.frame(load_csv_files(i, optional = TRUE))
if (any(grepl('BEV', as.matrix(df), ignore.case = TRUE))){
files_BEV <- c(files_BEV, i)
}
}
files_BEV <- unique(files_BEV)
files_BEV <- files_BEV[ !(files_BEV %in% c("energy/mappings/IEA_flow_sector", "Census_ind_VoS_state")) ]
files_BEV_filtered <- filter_csvs_by_exact_name(files_BEV, 'BEV')
files_BEV_filtered
library(infotheo)
install.packages("infotheo")
library(infotheo)
calc_mutual_info_matrix <- function(dfA, dfB) {
# Discretizar variables si no están discretas (infotheo::mutinformation requiere discretas)
# Aquí suponemos que las variables ya son categóricas o discretas, si no:
# dfA_discrete <- discretize(dfA)
# dfB_discrete <- discretize(dfB)
colsA <- colnames(dfA)
colsB <- colnames(dfB)
mi_mat <- matrix(0, nrow = length(colsA), ncol = length(colsB),
dimnames = list(colsA, colsB))
for (i in seq_along(colsA)) {
for (j in seq_along(colsB)) {
# Calcular MI, manejando NA
x <- dfA[[i]]
y <- dfB[[j]]
# Quitar NA para el cálculo conjunto
valid <- complete.cases(x, y)
if (sum(valid) > 1) {
mi <- mutinformation(x[valid], y[valid])
} else {
mi <- NA
}
mi_mat[i, j] <- mi
}
}
return(mi_mat)
}
pairs  <- build_csv_relations(tree = build_tree(), initial_files = files_BEV_filtered)
dbml_l <- build_dbml_from_pairs(pairs = pairs)
dbml <- dbml_l$dbml
relations <- dbml_l$relations
tables <- dbml_l$tables
tables
summary_relat <- as.data.frame(count_related_columns_pairwise(tables = tables, relations = relations) )
summary_relat
View(summary_relat)
files_BEV_filtered
info_mutua <- calc_mutual_info_matrix(get_csv_info("energy/mappings/UCD_techs")$df, get_csv_info("energy/mappings/UCD_techs_revised")$df)
df_joined <- inner_join(df1, df2, by = "id")
complete.cases
colsA <- colnames(dfA)
get_csv_info("energy/mappings/UCD_techs")$df, get_csv_info("energy/mappings/UCD_techs_revised")$df
df1 <- get_csv_info("energy/mappings/UCD_techs")$df
df2 <- get_csv_info("energy/mappings/UCD_techs_revised")$df
df1
View(df1)
colsA <- colnames(df1)
colsB <- colnames(df2)
colsA
x <- df1[[1]]
y <- df2[[1]]
x
y
complete.cases(x, y)
mi_mat <- matrix(0, nrow = length(colsA), ncol = length(colsB),
dimnames = list(colsA, colsB))
mutinformation(x, y)
entropy <- function(x) {
# Probabilidades de cada categoría
probs <- table(x) / length(x)
# Entropía en bits
-sum(probs * log2(probs))
}
entropy(df1)
entropy(df1$UCD_sector)
for (i in 1:ncol(df1)){
print(df1[[i]], entropy(df1[[i]]))
}
for (i in 1:ncol(df1)){
cat(df1[[i]], entropy(df1[[i]]))
}
for (i in 1:ncol(df1)){
cat(i, entropy(df1[[i]]))
}
entropy(as.matrix(df))
View(df1)
View(df2)
